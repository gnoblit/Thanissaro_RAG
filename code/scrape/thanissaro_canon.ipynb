{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import git\n",
    "import os\n",
    "\n",
    "import requests\n",
    "import unicodedata\n",
    "\n",
    "import re\n",
    "\n",
    "import jsonlines\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define master url, easiest to pull from mobile index\n",
    "master_url = \"https://www.dhammatalks.org/suttas/index_mobile.html\"\n",
    "books_of_interest = [\"DN\", \"MN\", \"SN\", \"AN\", \"KN\"]\n",
    "avoid = [\"histor\", \"endn\", \"bibl\", \"app\", \"ackn\", \"intro\", \"epi\", \"prol\", \"syll\"]\n",
    "\n",
    "# Try to connect\n",
    "response = requests.get(master_url)\n",
    "if response.status_code == 200:\n",
    "    # Redefine encoding\n",
    "    response.encoding = \"UTF-8\"\n",
    "    # Parse HTML content\n",
    "    soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "\n",
    "    # Find all links\n",
    "    links = soup.find_all(\"a\")\n",
    "    links = set(links)\n",
    "    # Define list to hold link dicts\n",
    "    link_dicts = []\n",
    "\n",
    "    for link in links:\n",
    "        href = link.get('href')\n",
    "\n",
    "        if any(book_of_note in href for book_of_note in books_of_interest) and not any(avoid_these in href for avoid_these in avoid):\n",
    "            href_split = link.get(\"href\").split(\"/\")\n",
    "            link_dicts.append(\n",
    "                {   \"book\": href_split[2],\n",
    "                    \"sub_book\": href_split[3] if len(href_split)> 4 else \"None\",\n",
    "                    \"url\": \"https://www.dhammatalks.org\" + link.get(\"href\"),\n",
    "                    \"sutta\": re.sub(\" +\", \" \", unicodedata.normalize(\"NFKD\", link.get_text()))#.replace(\"AÌ‚\", \"\"))\n",
    "                }\n",
    "            )\n",
    "    \n",
    "else:\n",
    "    print(\"Failed to connect to Thanissaro's Webpage\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get absolute file path\n",
    "git_repo = git.Repo(os.getcwd(), search_parent_directories=True)\n",
    "git_root = git_repo.git.rev_parse(\"--show-toplevel\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a864ecb2daf43af92d063e95798d061",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1420 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import string\n",
    "nonalpha = string.digits + string.punctuation + string.whitespace\n",
    "counter = 0\n",
    "# Now cycle through dictionary of links and append text\n",
    "# List to append failed request.get dictionaries to\n",
    "no_go = []\n",
    "\n",
    "for link_dict in tqdm(link_dicts):\n",
    "    response = requests.get(link_dict[\"url\"]) \n",
    "    # If failed response, append dictionary to no_go for further investigation\n",
    "    if response.status_code != 200:\n",
    "        no_go.append(link_dict)\n",
    "        print(f\"NOPE: {response}\")\n",
    "    # Successful response means pull out all text\n",
    "    else:\n",
    "        # Replace encoding\n",
    "        response.encoding = \"UTF-8\"\n",
    "        # Scrape\n",
    "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
    "        \n",
    "        # Define various fields of interest\n",
    "        # We will work from the outside in, slowly destroying as we go.\n",
    "        # Master sutta div\n",
    "        sutta_div_original = soup.find(\"div\", id=\"sutta\") ###****###\n",
    "        all_text = unicodedata.normalize(\"NFKD\", sutta_div_original.get_text()).strip() if sutta_div_original else None ###****###\n",
    "        \n",
    "        # Title in h1\n",
    "        title_attr = sutta_div_original.find(\"h1\") ###****###\n",
    "        title_text = re.sub('\\s{2,}', ' ', unicodedata.normalize(\"NFKD\", title_attr.get_text()).strip()) if title_attr else None ###****###\n",
    "        # Destroy h1\n",
    "        if title_attr:\n",
    "            title_attr.decompose()\n",
    "\n",
    "        # Now go to See also\n",
    "        see_also_tag_original = sutta_div_original.find_all(\"p\", {\"class\": \"seealso\"})\n",
    "        # If a list of see also tag exists\n",
    "        if see_also_tag_original:\n",
    "            ## Set to the final tag, the footnotes\n",
    "            see_also_tag_original = see_also_tag_original[-1] ###****###\n",
    "            ## Pull out text\n",
    "            see_also_text = unicodedata.normalize(\"NFKD\", see_also_tag_original.get_text().split(\"See also: \")[-1]).strip() ###****###\n",
    "\n",
    "            # Extract list of associated suttas\n",
    "            see_also_a_tags = see_also_tag_original.find_all(\"a\") ###****###\n",
    "            see_also_links = [\"https://www.dhammatalks.org\" + link.get('href') for link in see_also_a_tags] ###****###\n",
    "            see_also_suttas = [unicodedata.normalize(\"NFKD\", link.get_text()).strip() for link in see_also_a_tags] ###****###\n",
    "            see_also_text_and_link = [ (text_, link_) for text_, link_ in zip(see_also_suttas, see_also_links) ]\n",
    "\n",
    "            # Destroy see_also_tag\n",
    "            see_also_tag_original.decompose()\n",
    "        # If no list exists, just set everything to None, and no need to decompose see_also_tag\n",
    "        else:\n",
    "            see_also_tag_original = None \n",
    "            see_also_text = None ###****###\n",
    "            see_also_a_tags = None \n",
    "            see_also_links = None ###****###\n",
    "            see_also_suttas = None ###****###\n",
    "            see_also_text_and_link = None ###****###\n",
    "        \n",
    "        see_also_dict = {\n",
    "            # \"see_also_tag\":str(see_also_tag),\n",
    "            \"see_also_text\":see_also_text,\n",
    "            # \"see_also_a_tags\":str(see_also_a_tags),\n",
    "            \"see_also_links\":see_also_links,\n",
    "            \"see_also_suttas\":see_also_suttas,\n",
    "            \"see_also_text_and_link\": see_also_text_and_link\n",
    "        }\n",
    "\n",
    "        # Now go to note, second from bottom if it exists\n",
    "        note_tag_original = sutta_div_original.find(\"div\", {\"class\": \"note\"})\n",
    "\n",
    "        # If note list exists, set to final of list\n",
    "        if note_tag_original:\n",
    "            note_text = unicodedata.normalize(\"NFKD\", note_tag_original.get_text().strip()) ###****###\n",
    "\n",
    "            # Extract all paragraphs\n",
    "            # First destroy notetitle, we don't need it\n",
    "            note_title = note_tag_original.find(\"p\", {\"class\":\"notetitle\"})\n",
    "            if note_title:\n",
    "                note_title.decompose()\n",
    "            # Extract all paragraphs\n",
    "            note_paragraph_tags = note_tag_original.find_all(\"p\", {\"id\":re.compile(r'note')})\n",
    "            # Set note_counter to create dictionary of notes with associated text and links and associated resources\n",
    "            note_dicts = [] ###****###\n",
    "            note_counter = 0\n",
    "\n",
    "            if note_paragraph_tags:\n",
    "                # Go through each paragraph tag\n",
    "                for par_ in note_paragraph_tags:\n",
    "                    # Increment note_counter\n",
    "                    note_counter += 1\n",
    "                    # Find any links\n",
    "                    par_a_tags = par_.find_all(\"a\")\n",
    "\n",
    "                    # Set up dict to store\n",
    "                    temp_note_par_dict = {\n",
    "                        \"note_count\":\"note \" + str(note_counter), # Set note counter\n",
    "                        \"note_text\":unicodedata.normalize(\"NFKD\", par_.get_text()).strip().lstrip(nonalpha),\n",
    "                        \"links\":[\"https://www.dhammatalks.org\" + link.get(\"href\") for link in par_a_tags] if par_a_tags else [],\n",
    "                        \"links_text\":[unicodedata.normalize(\"NFKD\", link.get_text()).strip() for link in par_a_tags] if par_a_tags else [],\n",
    "                    }\n",
    "\n",
    "                    # Define all next siblings\n",
    "                    for sibling in par_.find_next_siblings():\n",
    "                        # If sibling is another paragraph tag, break\n",
    "                        if sibling in note_paragraph_tags:\n",
    "                            break\n",
    "                        \n",
    "                        # If no break, append text\n",
    "                        temp_note_par_dict[\"note_text\"] = temp_note_par_dict[\"note_text\"] + f\"\"\"\\n\\n{unicodedata.normalize(\"NFKD\", sibling.get_text()).strip()}\"\"\"\n",
    "                        # Check if any links\n",
    "                        sib_a_tags = sibling.find_all(\"a\")\n",
    "                        if sib_a_tags:\n",
    "                            temp_note_par_dict[\"links\"] = temp_note_par_dict[\"links\"] + [\"https://www.dhammatalks.org\" + link.get(\"href\") for link in sib_a_tags]\n",
    "                            # Add the link text to the dictionary\n",
    "                            temp_note_par_dict[\"links_text\"] = temp_note_par_dict[\"links_text\"] + [unicodedata.normalize(\"NFKD\", link.get_text()).strip() for link in sib_a_tags]\n",
    "                    # Now you've gone through all siblings, so append to list\n",
    "                    # Create final link, text zip\n",
    "                    temp_note_par_dict[\"link_and_text\"] = [(text_, link_) for  text_, link_ in zip(temp_note_par_dict[\"links_text\"], temp_note_par_dict[\"links\"])    ] if temp_note_par_dict[\"links_text\"] else []\n",
    "                    note_dicts.append(temp_note_par_dict)\n",
    "\n",
    "            ## Once done looking at all note pars, can decompose\n",
    "            note_tag_original.decompose()\n",
    "        else:\n",
    "            note_text = None\n",
    "            note_dicts = None\n",
    "            ################################## NEED TO FIGURE OUT HOW TO SET AS COLUMNS IN DF\n",
    "\n",
    "        # Left with intro and body, no way to split them nicely\n",
    "        \n",
    "        # Update ongoing text to drop title\n",
    "        ongoing_text = unicodedata.normalize(\"NFKD\", sutta_div_original.get_text()).strip() if sutta_div_original else None\n",
    "        \n",
    "        # Need to split on * * * or [ I ]\n",
    "        split_on_intro = re.split(\n",
    "            r'\\* \\* \\*|\\[ I \\]'\n",
    "            , ongoing_text\n",
    "        )\n",
    "        # For intro, pull out prior to split\n",
    "        intro_text = split_on_intro[0].strip() if len(split_on_intro) > 1 else None ###****###\n",
    "       \n",
    "        # Body text, pull out after split\n",
    "        sutta_text = split_on_intro[-1].strip() ###****###\n",
    "\n",
    "        # Also set up sutta_blocks\n",
    "        break_tags = sutta_div_original.find_all(\n",
    "            lambda el: (\"h\" in el.name) | (el.get_attribute_list('class')[0] == 'stars') \n",
    "            )\n",
    "        # Set up sutta_blocks to store.\n",
    "        # Key with be heading text, values will be list of elements text\n",
    "        sutta_blocks = {}\n",
    "        # Loop through each break\n",
    "        for break_ in break_tags:\n",
    "            # Initialize list of contained elements\n",
    "            values = []\n",
    "            # Loop through list of elements after the break tag\n",
    "            for sibling in break_.find_next_siblings():\n",
    "                # If element is a header or stars, it means we want to add a new key to our dict, so move on to next break_\n",
    "                if (\"h\" in sibling.name) or (sibling.get_attribute_list(\"class\")[0] == \"stars\"):\n",
    "                    break\n",
    "                # Otherwise, append the element to the list of elements for the current break_\n",
    "                values.append(unicodedata.normalize(\"NFKD\", sibling.text).strip())\n",
    "            # Append result to master_dict\n",
    "            sutta_blocks[break_.text] = values\n",
    "\n",
    "        ############################################### APPENDS ############################################\n",
    "        # Append everything to link dict\n",
    "        # link_dict[\"sutta_div_original\"] = str(sutta_div_original)\n",
    "        link_dict[\"all_text\"] = all_text\n",
    "        # link_dict[\"sutta_div_original\"] = str(sutta_div_original)\n",
    "        # Title\n",
    "        # link_dict[\"title_attr\"] = str(title_attr)\n",
    "        link_dict[\"title_text\"] = title_text\n",
    "        # Intro\n",
    "        link_dict[\"intro_text\"] = intro_text\n",
    "        # Sutta \n",
    "        link_dict[\"sutta_text\"] = sutta_text\n",
    "        link_dict[\"sutta_headings_with_text_dict\"] = sutta_blocks \n",
    "\n",
    "        # Notes\n",
    "        # link_dict[\"note_div\"] = str(note_div)\n",
    "        link_dict[\"note_text\"] = note_text\n",
    "        link_dict[\"note_dicts\"] = note_dicts\n",
    "\n",
    "        link_dict[\"note_text_dict\"] = {\n",
    "            el_['note_count']:el_['note_text'] for el_ in note_dicts\n",
    "        } if note_dicts else None\n",
    "        \n",
    "        # See also\n",
    "        for key_, value_ in see_also_dict.items():\n",
    "            link_dict[key_] = value_\n",
    "\n",
    "        # Write link dict\n",
    "        # Define write filename\n",
    "        link_dicts_write_filename = \"thanissaro_scraped.jsonl\"\n",
    "        # Define write filepath\n",
    "        link_dicts_write_path = git_root + \"/\" + \"data/web\" + \"/\" + link_dicts_write_filename\n",
    "        counter += 1\n",
    "\n",
    "        with jsonlines.open(link_dicts_write_path, mode='a') as writer:\n",
    "            writer.write(link_dict) \n",
    "print(f\"Count: {counter}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_thanissaro_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
